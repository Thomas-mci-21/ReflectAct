# ReAct Agent å®Œå…¨å¯¹é½è®¡åˆ’ï¼šReflAct â†’ MPOï¼ˆå®Œå…¨å¯¹é½ç‰ˆï¼‰

## ğŸ¯ ç›®æ ‡

**è®© ReAct Agent çš„å®ç°å®Œå…¨å¯¹é½åˆ° MPO**ï¼ŒåŒ…æ‹¬ï¼š
1. æ¶æ„å¯¹é½ï¼šEnvironment ç®¡ç† Stateï¼ŒAgent åªæ˜¯ LLM è°ƒç”¨å™¨
2. Prompt å¯¹é½ï¼šä½¿ç”¨ç›¸åŒçš„ prompt æ„å»ºæ–¹å¼
3. æ‰§è¡Œæµç¨‹å¯¹é½ï¼šä¸»å¾ªç¯ä¸ MPO å®Œå…¨ä¸€è‡´
4. Task ç®¡ç†å¯¹é½ï¼šä½¿ç”¨ Task å¯¹è±¡åŒ…è£…ä»»åŠ¡ä¿¡æ¯

## ğŸ” MPO çš„å®Œæ•´æ¶æ„åˆ†æ

### MPO çš„æ‰§è¡Œæµç¨‹

```
1. åŠ è½½ä»»åŠ¡ï¼ˆtasks/alfworld.pyï¼‰:
   - AlfWorldTask.load_tasks() åˆ›å»º ALFWorld ç¯å¢ƒ
   - éå†æ‰€æœ‰ä»»åŠ¡ï¼Œå¯¹æ¯ä¸ªä»»åŠ¡ï¼š
     - env.reset() è·å– obs å’Œ info
     - ä» info æå– game_fileï¼Œè¯†åˆ« task_type
     - åˆ›å»º AlfWorldTask(task_id, task_name, env, task_type, obs, workflow)
   - è¿”å› generator

2. ä¸»å¾ªç¯ï¼ˆmain.pyï¼‰:
   for task in all_tasks:
       # åˆ›å»º Environmentï¼ˆæ¥æ”¶ Task å¯¹è±¡ï¼‰
       env = AlfWorldEnv(task, **env_config)
       # env_config åŒ…å«: instruction_path, icl_path, icl_format, max_steps
       
       # é‡ç½®ç¯å¢ƒï¼ˆæ„å»º promptï¼‰
       observation, state = env.reset()
       
       # ReAct å¾ªç¯
       while not state.finished:
           llm_output = agent(state.history)  # Agent åªæ˜¯ LLM è°ƒç”¨å™¨
           observation, state = env.step(llm_output)  # ç¯å¢ƒè§£æå¹¶æ›´æ–° state
```

### MPO çš„å…³é”®ç»„ä»¶

1. **Task ç±»** (`tasks/alfworld.py`):
   - åŒ…å«ï¼š`task_id`, `task_name`, `env` (ALFWorldç¯å¢ƒ), `task_type`, `observation`, `workflow`
   - `load_tasks()` æ–¹æ³•åˆ›å»ºæ‰€æœ‰ä»»åŠ¡å¯¹è±¡

2. **BaseEnv** (`envs/base.py`):
   - åœ¨ `__init__` æ—¶è¯»å– `instruction_path` å’Œ `icl_path`
   - å­˜å‚¨ `self.instruction` å’Œ `self.raw_icl`
   - å®šä¹‰æŠ½è±¡æ–¹æ³• `reset()` å’Œ `step()`

3. **AlfWorldEnv** (`envs/alfworld_env.py`):
   - ç»§æ‰¿ `BaseEnv`
   - æ¥æ”¶ `task: AlfWorldTask` å¯¹è±¡
   - `reset()`: ä½¿ç”¨ `self.task.observation` å’Œ `self.task.task_type` æ„å»º prompt
   - `step()`: ä½¿ç”¨ `self.task.env` æ‰§è¡ŒåŠ¨ä½œï¼Œæ›´æ–° `self.state.history`

4. **Agent** (`agents/openai_agent.py`):
   - åªæ˜¯ LLM è°ƒç”¨å™¨
   - `__call__(messages) -> str`

## ğŸ“ å®Œå…¨å¯¹é½çš„è¯¦ç»†è®¡åˆ’

### Phase 1: æ·»åŠ  MPO çš„æ ¸å¿ƒç»„ä»¶

#### 1.1 åˆ›å»º State ç±»
**æ–‡ä»¶**: `utils/datatypes.py`ï¼ˆæ–°å»ºï¼‰

```python
# ä» MPO/utils/datatypes.py å®Œå…¨å¤åˆ¶
import enum
from copy import deepcopy
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict


class State:
    """This should contains everything needed to continue the conversation."""
    
    def __init__(
        self,
        reward: float = None,
        finished: bool = False,
        success: bool = False,
        terminate_reason: str = None,
    ):
        self.history: List[Dict[str, Any]] = []
        self.reward: float = reward
        self.finished: bool = finished
        self.success: bool = success
        self.terminate_reason: str = terminate_reason
        self.error: Optional[str] = None
        self.steps = 0
    
    # ... å…¶ä»–æ–¹æ³•ä» MPO å¤åˆ¶ ...
```

#### 1.2 åˆ›å»º BaseEnv åŸºç±»
**æ–‡ä»¶**: `envs/base.py`ï¼ˆæ–°å»ºï¼‰

```python
# ä» MPO/envs/base.py å®Œå…¨å¤åˆ¶
import json
from abc import ABC, abstractmethod
from typing import Tuple
from utils.datatypes import State


class BaseEnv(ABC):
    def __init__(
        self,
        instruction_path: str,
        icl_path: str,
        icl_format: str = "first",
        max_steps: int = 10,
        **kwargs,
    ):
        with open(instruction_path) as f:
            self.instruction = f.read()
        self.raw_icl = json.load(open(icl_path))
        self.icl_format = icl_format
        self.max_steps = max_steps

    @abstractmethod
    def step(self, llm_output: str) -> Tuple[str, State]:
        pass

    @abstractmethod
    def reset(self) -> Tuple[str, State]:
        pass
```

#### 1.3 åˆ›å»º Task ç±»
**æ–‡ä»¶**: `tasks/alfworld.py`ï¼ˆæ–°å»ºï¼‰

```python
# ä» MPO/tasks/alfworld.py å¤åˆ¶å¹¶è°ƒæ•´
import os
import json
import yaml
import logging
from typing import Iterable, Tuple

import alfworld
import alfworld.agents.environment as envs

from tasks.base import Task


logger = logging.getLogger("agent_eval")

PREFIXES = {
    "pick_and_place": "put",
    "pick_clean_then_place": "clean",
    "pick_heat_then_place": "heat",
    "pick_cool_then_place": "cool",
    "look_at_obj": "examine",
    "pick_two_obj": "puttwo",
}


class AlfWorldTask(Task):
    """Alfworld task instance."""
    
    task_name = "alfworld"
    
    def __init__(
        self,
        task_name: str,
        env: envs.AlfredTWEnv,
        task_type: str,
        obs: str,
        workflow: str = None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.task_name = task_name
        self.task_type = task_type
        self.observation = obs
        self.workflow = workflow
        self.env = env
    
    @classmethod
    def load_tasks(
        cls, 
        path: str, 
        workflow_path: str = None,
        split: str = "test",
        part_num: int = 1,
        part_idx: int = -1,
    ) -> Tuple[Iterable[Task], int]:
        """Load alfworld data and prompts from a directory."""
        os.environ["ALFWORLD_DATA"] = path
        
        with open(os.path.join(path, "base_config.yaml")) as f:
            config = yaml.safe_load(f)
        
        # Split following ReAct
        if split == 'train':
            split = "train"
            N_TASKS = 3553
        elif split == 'dev':
            split = "eval_in_distribution"
            N_TASKS = 140
        elif split == 'test':
            split = "eval_out_of_distribution"
            N_TASKS = 134
        
        env = getattr(alfworld.agents.environment, config["env"]["type"])(
            config, train_eval=split
        )
        assert isinstance(env, alfworld.agents.environment.AlfredTWEnv)
        env = env.init_env(batch_size=1)
        
        if workflow_path is not None:
            with open(workflow_path) as fr:
                raw_workflows = fr.readlines()
                workflows = [json.loads(w) for w in raw_workflows]
        else:
            workflows = [None] * N_TASKS
        
        if part_num > 1:
            assert part_idx != -1
            part_inst_num = [N_TASKS // part_num] * part_num
            part_inst_num[-1] += N_TASKS % part_num
            env.skip(sum(part_inst_num[:part_idx]))
            workflows = workflows[sum(part_inst_num[:part_idx]):sum(part_inst_num[:part_idx+1])]
            N_TASKS = part_inst_num[part_idx]
        
        obs_2_workflow = {}
        for item in workflows:
            if item is None:
                continue
            obs = item['task']
            workflow = item['workflow']
            obs_2_workflow[obs] = workflow
        
        def generator():
            for idx in range(N_TASKS):
                obs, info = env.reset()
                obs = "\n".join(obs[0].split("\n\n")[1:])
                game_file = info["extra.gamefile"][0]
                
                name = "/".join(game_file.split("/")[-3:-1])
                
                task_type = None
                for _, (k, v) in enumerate(PREFIXES.items()):
                    if name.startswith(k):
                        task_type = k
                        break
                assert task_type is not None, f"Task type not found for {name}"
                
                yield cls(
                    task_id=idx,
                    task_name=name,
                    env=env,
                    task_type=task_type,
                    obs=obs,
                    workflow=obs_2_workflow.get(obs, None),
                )
        
        return generator(), N_TASKS
```

#### 1.4 åˆ›å»º Task åŸºç±»
**æ–‡ä»¶**: `tasks/base.py`ï¼ˆæ–°å»ºï¼‰

```python
# ä» MPO/tasks/base.py å®Œå…¨å¤åˆ¶
import json
import logging
from abc import ABC
from typing import Any, List, Tuple

logger = logging.getLogger("agent_eval")


class Task(ABC):
    """Base class for a task instance."""
    
    task_name: str = "base"
    
    def __init__(self, **kwargs) -> None:
        self.task_id: Any = kwargs.get("task_id", None)
        self.metadata = {}
```

#### 1.5 å¤åˆ¶ prompt ç›¸å…³æ–‡ä»¶
- `prompt/instructions/alfworld_inst.txt` - ä» MPO å¤åˆ¶
- `prompt/icl_examples/alfworld_icl.json` - ä» MPO å¤åˆ¶
- `prompt/templates.py` - ä» MPO å¤åˆ¶
- `prompt/__init__.py` - æ–°å»ºï¼Œå¯¼å‡º `prompt_with_icl`

#### 1.6 åˆ›å»ºå·¥å…·å‡½æ•°
**æ–‡ä»¶**: `utils/task_utils.py`ï¼ˆæ–°å»ºï¼‰

```python
"""Utility functions for task handling (aligned with MPO)."""

def process_ob(ob):
    """Process observation (aligned with MPO)."""
    if ob.startswith('You arrive at loc '):
        ob = ob[ob.find('. ')+2:]
    return ob
```

### Phase 2: åˆ›å»º MPO é£æ ¼çš„ Environment

#### 2.1 åˆ›å»ºæ–°çš„ Environment ç±»
**æ–‡ä»¶**: `environment/alfworld_env_mpo.py`ï¼ˆæ–°å»ºï¼‰

**ç­–ç•¥**ï¼šåˆ›å»ºæ–°çš„ MPO é£æ ¼çš„ Environment ç±»ï¼Œå®Œå…¨å¯¹é½ MPO

```python
"""
ALFWorld environment wrapper - MPO-aligned implementation.
Completely aligned with MPO architecture.
"""
import re
import json
import logging
from typing import Any, Dict, List, Tuple

from envs.base import BaseEnv
from tasks.alfworld import AlfWorldTask
from prompt.templates import prompt_with_icl
from utils.datatypes import State
from utils.task_utils import process_ob


logger = logging.getLogger("agent_eval")


class AlfWorldEnvMPO(BaseEnv):
    """
    ALFWorld environment wrapper aligned with MPO.
    
    Key characteristics (matching MPO):
    - Inherits from BaseEnv
    - Receives Task object
    - Manages State object
    - reset() returns (observation: str, state: State)
    - step(llm_output: str) returns (observation: str, state: State)
    """
    
    def __init__(
        self,
        task: AlfWorldTask,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.task: AlfWorldTask = task
        self.env = task.env
        self.state = State()
        self.bad_steps = 0
        self.max_bad_steps = 50
    
    def parse_action(self, llm_output: str) -> str:
        """Parse action from LLM output (aligned with MPO)."""
        llm_output = llm_output.strip()
        pattern = re.compile(r"Action:\s?(.*)", re.DOTALL)
        action = re.findall(pattern, llm_output)[0]
        put_action = re.findall(r"put\s+(.*)\s+[io]n\s+(.*)", action)
        if put_action:
            action = f"put {put_action[0][0]} in/on {put_action[0][1]}"
        assert action is not None
        return action
    
    def conduct_action(self, action: str):
        """Execute action and return observation, reward, done."""
        observation, reward, done, info = self.env.step([action])
        observation, reward, done = process_ob(observation[0]), info['won'][0], done[0]
        return observation, reward, done
    
    def step(self, llm_output: str) -> Tuple[str, State]:
        """
        MPO-style step: receives LLM output, parses action, executes, updates state.
        
        Args:
            llm_output: Raw LLM output string
        
        Returns:
            Tuple of (observation, state)
        """
        # Add LLM output to history
        self.state.history.append({
            "role": "assistant",
            "content": llm_output
        })
        
        # Parse and execute action
        try:
            action = self.parse_action(llm_output)
            observation, reward, done = self.conduct_action(action)
        except Exception as e:
            self.state.success = False
            self.state.finished = False
            self.state.reward = 0
            observation = f"Observation: Error Input. Your input must contains 'Action: '"
            self.state.history.append({
                "role": "user",
                "content": observation,
            })
            self.state.steps += 1
            self.bad_steps += 1
            if self.state.steps >= self.max_steps or self.bad_steps >= self.max_bad_steps:
                self.state.finished = True
                self.state.success = False
                self.state.terminate_reason = "max_steps"
                self.state.reward = 0
            return observation, self.state
        
        # Process observation
        observation = f"Observation: {observation}"
        
        # Add observation to history
        self.state.history.append({
            "role": "user",
            "content": observation,
        })
        
        self.state.steps += 1
        if self.state.steps >= self.max_steps or self.bad_steps >= self.max_bad_steps:
            self.state.finished = True
            self.state.success = False
            self.state.terminate_reason = "max_steps"
            self.state.reward = reward
        
        if done:
            self.state.finished = True
            self.state.success = True
            self.state.terminate_reason = "success"
            self.state.reward = reward
        
        return observation, self.state
    
    def reset(self) -> Tuple[str, State]:
        """
        MPO-style reset: builds prompt and returns (observation, state).
        
        Returns:
            Tuple of (observation, state) where state contains history
        """
        self.state = State()
        cur_task = self.task.observation
        
        # Build prompt using MPO's template
        # Note: MPO uses args.incorporation_type, we default to "query" for ReAct
        observation, messages = prompt_with_icl(
            instruction=self.instruction, 
            raw_icl=self.raw_icl[self.task.task_type], 
            cur_task=cur_task, 
            icl_num=1,
            workflow=self.task.workflow if hasattr(self.task, 'workflow') else None,
        )
        
        # Use 'first' format: all content in one user message
        if self.icl_format == 'first':
            self.state.history.append({
                "role": "user",
                "content": observation,
            })
        elif self.icl_format == 'conversation':
            self.state.history = messages
        
        return observation, self.state
```

### Phase 3: é‡å†™ ReAct Agent ä¸ºç®€å•çš„ LLM è°ƒç”¨å™¨

#### 3.1 å®Œå…¨é‡å†™ `agents/react_agent.py`

```python
"""
ReAct Agent - MPO-aligned implementation.
Completely aligned with MPO: Agent is just an LLM caller.
"""
import logging
import os
import backoff
import openai
from openai import OpenAI
import config

logger = logging.getLogger("agent_eval")


class ReActAgent:
    """
    ReAct Agent aligned with MPO implementation.
    
    Key characteristics (matching MPO):
    - Agent is just an LLM caller
    - No state management
    - No action parsing
    - Simple __call__(messages) -> str interface
    """
    
    agent_type = "react"
    
    def __init__(self, config_dict: dict = None, verbose: bool = True):
        """
        Initialize agent.
        
        Args:
            config_dict: Agent config dict (for compatibility with MPO)
            verbose: Whether to print to terminal
        """
        self.verbose = verbose
        
        # Use config_dict if provided (MPO style), otherwise use config module
        if config_dict:
            self.config = config_dict
            self.client = OpenAI(
                base_url=config_dict.get("api_base", None),
                api_key=config_dict.get("api_key", os.environ.get("OPENAI_API_KEY")),
            )
            self.model_name = config_dict["model_name"]
            self.max_completion_tokens = config_dict.get("max_completion_tokens", 512)
            self.temperature = config_dict.get("temperature", 0)
        else:
            # ReflAct style (use config module)
            self.client = OpenAI(
                api_key=config.OPENAI_API_KEY,
                base_url=config.OPENAI_BASE_URL
            )
            self.model_name = config.OPENAI_MODEL
            self.max_completion_tokens = 512
            self.temperature = config.TEMPERATURE
        
        self.stop_words = ["\nObservation:", "\nTask:", "\n---"]
    
    @backoff.on_exception(
        backoff.fibo,
        (
            openai.APIError,
            openai.Timeout,
            openai.RateLimitError,
            openai.APIConnectionError,
        ),
    )
    def __call__(self, messages) -> str:
        """
        Call LLM with messages, return raw LLM output.
        
        Args:
            messages: List of message dicts with 'role' and 'content'
        
        Returns:
            Raw LLM output string
        """
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=self.max_completion_tokens,
            temperature=self.temperature,
            stop=self.stop_words,
        )
        return response.choices[0].message.content
```

### Phase 4: ä¿®æ”¹ä¸»æ‰§è¡Œå¾ªç¯

#### 4.1 ä¿®æ”¹ `run_experiment.py`

**ç­–ç•¥**ï¼šä¸º ReAct Agent æ·»åŠ ä¸“é—¨çš„æ‰§è¡Œå‡½æ•°ï¼Œå®Œå…¨å¯¹é½ MPO

```python
def run_react_agent_mpo_style(
    num_tasks: int = 134,
    start_task: int = 0,
    verbose: bool = True,
) -> Dict:
    """
    Run ReAct Agent using MPO-style execution loop.
    
    This function uses MPO's architecture:
    - Task objects are loaded first
    - Environment manages State
    - Agent is just an LLM caller
    - Main loop: env.reset() -> agent(state.history) -> env.step(llm_output)
    """
    import os
    from tasks.alfworld import AlfWorldTask
    from environment.alfworld_env_mpo import AlfWorldEnvMPO
    from agents.react_agent import ReActAgent
    from utils.logger import save_summary, Colors, log_task_start, log_task_end, save_trajectory
    
    # Load tasks (MPO style)
    # Determine ALFWorld data path
    alfworld_data = os.getenv('ALFWORLD_DATA', os.path.expanduser('~/.cache/alfworld'))
    if not os.path.exists(alfworld_data):
        alfworld_data = "data/alfworld"  # Fallback
    
    all_tasks, n_tasks = AlfWorldTask.load_tasks(
        path=alfworld_data,
        workflow_path=None,
        split="test",  # Use test split
        part_num=1,
        part_idx=-1,
    )
    
    # Limit number of tasks
    task_list = list(all_tasks)[start_task:start_task + num_tasks]
    
    # Environment config (MPO style)
    base_dir = os.path.dirname(os.path.dirname(__file__))
    env_config = {
        "instruction_path": os.path.join(base_dir, "prompt", "instructions", "alfworld_inst.txt"),
        "icl_path": os.path.join(base_dir, "prompt", "icl_examples", "alfworld_icl.json"),
        "icl_format": "first",
        "max_steps": 30,
    }
    
    # Initialize agent
    agent = ReActAgent(verbose=verbose)
    
    results = []
    
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}Running REACT Agent (MPO-aligned) on {len(task_list)} tasks{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    for task in task_list:
        try:
            # Create environment for this task (MPO style)
            env = AlfWorldEnvMPO(task, **env_config)
            
            # Reset environment (builds prompt)
            observation, state = env.reset()
            
            task_desc = f"Task {task.task_id}: {task.task_name}"
            log_task_start(task.task_id, task_desc, 'react', verbose)
            
            if verbose:
                print(f"\n{Colors.YELLOW}Initial Prompt:{Colors.RESET}\n{observation[:500]}...")
            
            # Main loop (MPO style)
            while not state.finished and state.steps < env_config["max_steps"]:
                try:
                    # Agent just calls LLM
                    llm_output = agent(state.history)
                    
                    if verbose:
                        print(f"\n{Colors.GREEN}Agent Output:{Colors.RESET}\n{llm_output}")
                    
                    # Environment parses, executes, and updates state
                    observation, state = env.step(llm_output)
                    
                    if verbose and not state.finished:
                        print(f"\n{Colors.BLUE}Observation:{Colors.RESET}\n{observation}")
                        
                except Exception as e:
                    if verbose:
                        print(f"{Colors.RED}Error: {e}{Colors.RESET}")
                    state.success = False
                    state.finished = True
                    state.terminate_reason = "agent_error"
                    break
            
            # Log and save
            success = state.success
            log_task_end(success, state.steps, verbose)
            
            # Convert state.history to trajectory format (for compatibility)
            trajectory = []
            for i, msg in enumerate(state.history):
                if msg['role'] == 'assistant':
                    # Parse assistant message for trajectory
                    from utils.llm import parse_thought_or_reflection
                    reasoning_type, reasoning, action = parse_thought_or_reflection(msg['content'])
                    prev_obs = state.history[i-1]['content'] if i > 0 and state.history[i-1]['role'] == 'user' else ''
                    trajectory.append({
                        'step': len(trajectory) + 1,
                        'observation': prev_obs.replace('Observation: ', '') if prev_obs.startswith('Observation: ') else prev_obs,
                        'reasoning_type': reasoning_type,
                        'reasoning': reasoning,
                        'action': action,
                    })
            
            save_trajectory(
                task_id=task.task_id,
                task_description=task_desc,
                agent_type='react',
                trajectory=trajectory,
                success=success,
                save=config.SAVE_RESULTS
            )
            
            results.append({
                "task_id": task.task_id,
                "success": success,
                "num_steps": state.steps,
            })
            
        except Exception as e:
            print(f"{Colors.RED}Error on task {task.task_id}: {e}{Colors.RESET}")
            import traceback
            traceback.print_exc()
            results.append({
                "task_id": task.task_id,
                "success": False,
                "num_steps": 0,
                "error": str(e),
            })
    
    # Save summary
    summary = save_summary('react', results)
    
    return summary


def run_agent_experiments(
    agent_type: str,
    num_tasks: int = 134,
    start_task: int = 0,
    verbose: bool = True,
) -> Dict:
    """
    Run experiments for a single agent type.
    For ReAct agent, uses MPO-style execution.
    """
    # Special handling for ReAct agent
    if agent_type == "react":
        return run_react_agent_mpo_style(num_tasks, start_task, verbose)
    
    # Other agents use original style
    # ... åŸæœ‰ä»£ç ä¿æŒä¸å˜ ...
    from environment.alfworld_env import ALFWorldEnv
    from agents import (
        NoThinkingAgent,
        PlanAndActAgent,
        ReflActAgent,
    )
    
    if agent_type not in {
        "nothinking": NoThinkingAgent,
        "plan_and_act": PlanAndActAgent,
        "reflact": ReflActAgent,
    }:
        raise ValueError(f"Unknown agent type: {agent_type}")
    
    # ... åŸæœ‰å®ç° ...
```

### Phase 5: åˆ›å»ºå¿…è¦çš„ç›®å½•å’Œæ–‡ä»¶

#### 5.1 ç›®å½•ç»“æ„
```
reflectact/
â”œâ”€â”€ envs/              # æ–°å»ºï¼ˆæ³¨æ„æ˜¯å¤æ•°ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ base.py
â”œâ”€â”€ tasks/             # æ–°å»º
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py
â”‚   â””â”€â”€ alfworld.py
â”œâ”€â”€ prompt/            # æ–°å»ºï¼ˆæ³¨æ„æ˜¯å•æ•°ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ instructions/
â”‚   â”‚   â””â”€â”€ alfworld_inst.txt
â”‚   â”œâ”€â”€ icl_examples/
â”‚   â”‚   â””â”€â”€ alfworld_icl.json
â”‚   â””â”€â”€ templates.py
â””â”€â”€ environment/
    â””â”€â”€ alfworld_env_mpo.py  # æ–°å»ºï¼ˆMPO é£æ ¼ï¼‰
```

## ğŸ¯ å®æ–½æ­¥éª¤æ€»ç»“

### Step 1: åˆ›å»ºæ ¸å¿ƒç»„ä»¶ âœ…
1. åˆ›å»º `utils/datatypes.py` - State ç±»
2. åˆ›å»º `envs/base.py` - BaseEnv åŸºç±»
3. åˆ›å»º `tasks/base.py` - Task åŸºç±»
4. åˆ›å»º `tasks/alfworld.py` - AlfWorldTask ç±»
5. åˆ›å»º `utils/task_utils.py` - å·¥å…·å‡½æ•°

### Step 2: å¤åˆ¶ prompt æ–‡ä»¶ âœ…
1. åˆ›å»º `prompt/` ç›®å½•ç»“æ„
2. å¤åˆ¶ `alfworld_inst.txt`
3. å¤åˆ¶ `alfworld_icl.json`
4. å¤åˆ¶ `templates.py`

### Step 3: åˆ›å»º MPO é£æ ¼çš„ Environment âœ…
1. åˆ›å»º `environment/alfworld_env_mpo.py`
2. å®Œå…¨å¯¹é½ MPO çš„å®ç°

### Step 4: é‡å†™ ReAct Agent âœ…
1. å®Œå…¨é‡å†™ä¸ºç®€å•çš„ LLM è°ƒç”¨å™¨
2. å®ç° `__call__(messages) -> str` æ¥å£

### Step 5: ä¿®æ”¹ä¸»å¾ªç¯ âœ…
1. æ·»åŠ  `run_react_agent_mpo_style()` å‡½æ•°
2. ä¿®æ”¹ `run_agent_experiments()` è·¯ç”±
3. ä¿æŒå…¶ä»– Agent ä½¿ç”¨åŸæœ‰æ–¹å¼

### Step 6: æµ‹è¯• âœ…
1. æµ‹è¯• ReAct Agent åŠŸèƒ½
2. å¯¹æ¯”è¾“å‡ºç¡®ä¿å®Œå…¨å¯¹é½

## âœ… å¯¹é½æ£€æŸ¥æ¸…å•

å®Œæˆåæ£€æŸ¥ä»¥ä¸‹é¡¹ç›®æ˜¯å¦ä¸ MPO å®Œå…¨å¯¹é½ï¼š
- [x] **æ¶æ„å¯¹é½**ï¼šEnvironment ç®¡ç† Stateï¼ŒAgent åªæ˜¯ LLM è°ƒç”¨å™¨
- [x] **Task ç®¡ç†å¯¹é½**ï¼šä½¿ç”¨ Task å¯¹è±¡åŒ…è£…ä»»åŠ¡ä¿¡æ¯
- [x] **BaseEnv å¯¹é½**ï¼šEnvironment ç»§æ‰¿ BaseEnvï¼Œè¯»å– prompt æ–‡ä»¶
- [x] **ä¸ä½¿ç”¨ system role**ï¼šæ‰€æœ‰å†…å®¹åœ¨ user message ä¸­
- [x] **æŒ‰ä»»åŠ¡ç±»å‹é€‰æ‹© 1 ä¸ª few-shot ç¤ºä¾‹**
- [x] **ä½¿ç”¨ç›¸åŒçš„ prompt æ¨¡æ¿**ï¼ˆ`prompt_with_icl()`ï¼‰
- [x] **ä½¿ç”¨ç›¸åŒçš„æŒ‡ä»¤æ–‡æœ¬**
- [x] **ä½¿ç”¨ç›¸åŒçš„ ICL ç¤ºä¾‹æ–‡ä»¶**
- [x] **å¯¹è¯å†å²æ ¼å¼ç›¸åŒ**ï¼ˆ`state.history` æ˜¯ `List[Dict]`ï¼‰
- [x] **Action è§£ææ–¹å¼ç›¸åŒ**ï¼ˆåŒ…å« `put` å¤„ç†ï¼‰
- [x] **è§‚å¯Ÿå¤„ç†æ–¹å¼ç›¸åŒ**ï¼ˆ`process_ob()`ï¼‰
- [x] **æ‰§è¡Œæµç¨‹ç›¸åŒ**ï¼š`task.load_tasks()` -> `env.reset()` -> `agent(state.history)` -> `env.step(llm_output)`

## ğŸ’¡ å…³é”®ä¼˜åŠ¿

1. **å®Œå…¨å¯¹é½**ï¼šæ¶æ„å’Œæ‰§è¡Œæµç¨‹ä¸ MPO 100% ä¸€è‡´
2. **ä¸å½±å“å…¶ä»– Agent**ï¼šå…¶ä»– Agent ç»§ç»­ä½¿ç”¨åŸæœ‰æ¶æ„
3. **ä»£ç æ¸…æ™°**ï¼šèŒè´£åˆ†ç¦»æ˜ç¡®
4. **æ˜“äºç»´æŠ¤**ï¼šReAct Agent ä»£ç éå¸¸ç®€æ´
5. **å¯æ‰©å±•**ï¼šå¦‚æœéœ€è¦ï¼Œå¯ä»¥è½»æ¾æ·»åŠ å…¶ä»– MPO é£æ ¼çš„åŠŸèƒ½

## ğŸ”§ æ–‡ä»¶ä¿®æ”¹æ¸…å•

### æ–°å¢æ–‡ä»¶
- [ ] `utils/datatypes.py` - State ç±»
- [ ] `envs/base.py` - BaseEnv åŸºç±»
- [ ] `tasks/base.py` - Task åŸºç±»
- [ ] `tasks/alfworld.py` - AlfWorldTask ç±»
- [ ] `tasks/__init__.py` - å¯¼å‡º Task ç±»
- [ ] `envs/__init__.py` - å¯¼å‡º BaseEnv
- [ ] `utils/task_utils.py` - ä»»åŠ¡å·¥å…·å‡½æ•°
- [ ] `prompt/instructions/alfworld_inst.txt`
- [ ] `prompt/icl_examples/alfworld_icl.json`
- [ ] `prompt/templates.py`
- [ ] `prompt/__init__.py`
- [ ] `environment/alfworld_env_mpo.py` - MPO é£æ ¼çš„ Environment

### ä¿®æ”¹æ–‡ä»¶
- [ ] `agents/react_agent.py` - å®Œå…¨é‡å†™ä¸º LLM è°ƒç”¨å™¨
- [ ] `run_experiment.py` - æ·»åŠ  MPO é£æ ¼æ‰§è¡Œå‡½æ•°

### ä¿æŒä¸å˜
- [x] å…¶ä»– Agentï¼ˆNoThinking, PlanAndAct, ReflActï¼‰
- [x] åŸæœ‰ Environmentï¼ˆ`alfworld_env.py`ï¼‰
- [x] å…¶ä»–æ‰€æœ‰æ–‡ä»¶

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å®Œå…¨å¯¹é½**ï¼šå³ä½¿å¯¼è‡´ ReflAct å˜åŒ–è¾ƒå¤§ï¼Œä¹Ÿè¦ç¡®ä¿ä¸ MPO å®Œå…¨å¯¹é½
2. **å‘åå…¼å®¹**ï¼šåŸæœ‰æ¥å£ä¿æŒä¸å˜ï¼Œåªæ·»åŠ æ–°æ–¹æ³•å’Œç±»
3. **æµ‹è¯•å……åˆ†**ï¼šç¡®ä¿åŠŸèƒ½æ­£å¸¸ä¸”è¾“å‡ºå¯¹é½
4. **è·¯å¾„é…ç½®**ï¼šç¡®ä¿æ‰€æœ‰æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–é…ç½®æ–‡ä»¶ï¼‰
5. **ä¾èµ–ç®¡ç†**ï¼šç¡®ä¿æ‰€æœ‰ä¾èµ–ï¼ˆå¦‚ `backoff`ï¼‰å·²å®‰è£…

## ğŸ“Š å…³é”®å·®å¼‚ä¿®æ­£

### ä¿®æ­£ 1: Task å¯¹è±¡ç®¡ç†
- **åŸè®¡åˆ’é”™è¯¯**ï¼šEnvironment è‡ªå·±ç®¡ç†ä»»åŠ¡
- **ä¿®æ­£**ï¼šä½¿ç”¨ Task å¯¹è±¡ï¼ˆAlfWorldTaskï¼‰ï¼ŒEnvironment æ¥æ”¶ Task

### ä¿®æ­£ 2: BaseEnv ç»§æ‰¿
- **åŸè®¡åˆ’é”™è¯¯**ï¼šEnvironment ä¸ç»§æ‰¿ BaseEnv
- **ä¿®æ­£**ï¼šåˆ›å»º BaseEnv åŸºç±»ï¼ŒMPO é£æ ¼çš„ Environment ç»§æ‰¿å®ƒ

### ä¿®æ­£ 3: Prompt æ–‡ä»¶è¯»å–
- **åŸè®¡åˆ’é”™è¯¯**ï¼šåœ¨ reset() æ—¶è¯»å–
- **ä¿®æ­£**ï¼šåœ¨ BaseEnv.__init__() æ—¶è¯»å–ï¼ˆä¸ MPO ä¸€è‡´ï¼‰

### ä¿®æ­£ 4: è§‚å¯Ÿå¤„ç†
- **åŸè®¡åˆ’é”™è¯¯**ï¼šåœ¨ Environment å¤–éƒ¨å¤„ç†
- **ä¿®æ­£**ï¼šåœ¨ `conduct_action()` ä¸­å¤„ç†ï¼ˆä¸ MPO ä¸€è‡´ï¼‰

### ä¿®æ­£ 5: ä¸»å¾ªç¯
- **åŸè®¡åˆ’é”™è¯¯**ï¼šEnvironment è‡ªå·±ç®¡ç†ä»»åŠ¡ç´¢å¼•
- **ä¿®æ­£**ï¼šå…ˆåŠ è½½æ‰€æœ‰ Task å¯¹è±¡ï¼Œç„¶åå¯¹æ¯ä¸ª Task åˆ›å»º Environmentï¼ˆä¸ MPO ä¸€è‡´ï¼‰

### ä¿®æ­£ 6: Evaluation é€»è¾‘
- **åŸè®¡åˆ’é—æ¼**ï¼šæœªè€ƒè™‘ evaluation å¯¹é½
- **ä¿®æ­£**ï¼šæ·»åŠ  Phase 6ï¼Œå®Œå…¨å¯¹é½ MPO çš„ evaluation å®ç°
  - Reward ä½¿ç”¨ `info['won'][0]` è€Œé `scores[0]`
  - Success åˆ¤æ–­ï¼š`done=True` â†’ `success=True`
  - è¶…æ—¶å¤„ç†ï¼šè®¾ç½® `success=False`ï¼Œ`terminate_reason="max_steps"`
  - è®¡ç®— `average_reward` æŒ‡æ ‡
  - ä¿å­˜å®Œæ•´çš„ State å¯¹è±¡

## Phase 6: Evaluation å¯¹é½

### 6.1 MPO çš„ Evaluation å®ç°åˆ†æ

#### Success/Reward åˆ¤æ–­ï¼ˆ`envs/alfworld_env.py`ï¼‰

```python
def conduct_action(self, action: str):
    observation, reward, done, info = self.env.step([action])
    # å…³é”®ï¼šreward æ¥è‡ª ALFWorld çš„ info['won'][0]
    observation, reward, done = process_ob(observation[0]), info['won'][0], done[0]
    return observation, reward, done

def step(self, llm_output: str) -> Tuple[str, State]:
    # ... è§£æå’Œæ‰§è¡Œ action ...
    
    # Success åˆ¤æ–­é€»è¾‘
    if done:
        self.state.finished = True
        self.state.success = True  # done=True æ—¶ success=True
        self.state.reward = reward  # reward = info['won'][0]
        self.state.terminate_reason = "success"
    
    # è¶…æ—¶å¤„ç†
    if self.state.steps >= self.max_steps or self.bad_steps >= self.max_bad_steps:
        self.state.finished = True
        self.state.success = False
        self.state.terminate_reason = "max_steps"
        self.state.reward = reward  # å¯èƒ½æ˜¯ 0
```

#### ç»“æœä¿å­˜å’ŒæŒ‡æ ‡è®¡ç®—ï¼ˆ`main.py`ï¼‰

```python
# ä¿å­˜æ¯ä¸ªä»»åŠ¡çš„ state
state_list.append(state)
json.dump(state.to_dict(), open(os.path.join(output_path, f"{task.task_id}.json"), 'w'), indent=4)

# è®¡ç®—æŒ‡æ ‡
reward_list = []
success_list = []
for state in state_list:
    if state.reward is not None:
        reward_list.append(state.reward)
    success_list.append(state.success)

# è¾“å‡ºæŒ‡æ ‡
if len(reward_list) != 0:
    logger.info(f"Average reward: {sum(reward_list)/len(success_list):.4f}")
logger.info(f"Success rate: {sum(success_list)/len(success_list):.4f}")
```

### 6.2 reflectact çš„ Evaluation å¯¹é½å®ç°

#### å·²å®Œæˆçš„ä¿®æ”¹ï¼š

1. **`environment/alfworld_env.py`** - ä¿®æ”¹ step() æ–¹æ³•ä½¿ç”¨ `info['won']`
   - å·²æ›´æ–°ï¼šreward ä¼˜å…ˆä½¿ç”¨ `info['won'][0]`ï¼Œå›é€€åˆ° `scores[0]`

2. **`utils/logger.py`** - æ·»åŠ  MPO é£æ ¼çš„æŒ‡æ ‡è®¡ç®—
   - å·²æ·»åŠ ï¼š`save_state_result()` å‡½æ•°
   - å·²æ›´æ–°ï¼š`save_summary()` æ”¯æŒ `states` å‚æ•°å’Œ `average_reward` è®¡ç®—

3. **`run_experiment.py`** - å®ç° MPO é£æ ¼çš„ evaluation
   - å·²æ·»åŠ ï¼š`run_react_agent_mpo_style()` å‡½æ•°
   - å·²å®ç°ï¼šä¿å­˜ State å¯¹è±¡ï¼Œæ”¶é›† statesï¼Œè®¡ç®— success_rate å’Œ average_reward

4. **`environment/alfworld_env_mpo.py`** - MPO é£æ ¼çš„ Environment
   - å·²å®ç°ï¼šå®Œå…¨å¯¹é½çš„ evaluation é€»è¾‘ï¼ˆåœ¨ `step()` å’Œ `conduct_action()` ä¸­ï¼‰

## Evaluation å¯¹é½æ£€æŸ¥æ¸…å•

å®Œæˆåæ£€æŸ¥ä»¥ä¸‹é¡¹ç›®æ˜¯å¦ä¸ MPO å®Œå…¨å¯¹é½ï¼š

- [x] **Reward æ¥æº**ï¼šä½¿ç”¨ `info['won'][0]` ä½œä¸º rewardï¼ˆè€Œé `scores[0]`ï¼‰
- [x] **Success åˆ¤æ–­**ï¼š`done=True` æ—¶ `success=True`ï¼Œè¶…æ—¶æ—¶ `success=False`
- [x] **è¶…æ—¶å¤„ç†**ï¼š`max_steps` æˆ– `max_bad_steps` è¶…æ—¶ â†’ `success=False`ï¼Œ`terminate_reason="max_steps"`
- [x] **é”™è¯¯å¤„ç†**ï¼šè§£æå¤±è´¥æ—¶å¢åŠ  `bad_steps`ï¼Œè¾¾åˆ°ä¸Šé™æ—¶æ ‡è®°å¤±è´¥
- [x] **State ä¿å­˜**ï¼šä¿å­˜å®Œæ•´çš„ State å¯¹è±¡ï¼ˆåŒ…å« history, reward, success, steps, terminate_reasonï¼‰
- [x] **æŒ‡æ ‡è®¡ç®—**ï¼šè®¡ç®— `success_rate` å’Œ `average_reward`
- [x] **ç»“æœæ–‡ä»¶æ ¼å¼**ï¼šæ¯ä¸ªä»»åŠ¡çš„ JSON æ–‡ä»¶åŒ…å«å®Œæ•´çš„ state ä¿¡æ¯ï¼ˆä¸ MPO æ ¼å¼ä¸€è‡´ï¼‰
