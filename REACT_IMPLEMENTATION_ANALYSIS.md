# ReAct Agent åœ¨ ALFWorld ä¸Šçš„å®ç°åˆ†æ (ReflAct ä»£ç åº“)

## ğŸ“‹ æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªåŸºäº ReflAct è®ºæ–‡çš„ä»£ç åº“ï¼Œå®ç°äº†å¤šç§ Agent æ–¹æ³•ï¼ˆNoThinking, ReAct, Plan-and-Act, ReflActï¼‰ï¼Œè¿™é‡Œé‡ç‚¹åˆ†æ **ReAct Agent çš„å®ç°**ã€‚

## ğŸ—ï¸ æ¶æ„æ¦‚è§ˆ

```
reflectact/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base_agent.py        # åŸºç±»ï¼Œå®šä¹‰é€šç”¨é€»è¾‘
â”‚   â””â”€â”€ react_agent.py       # ReAct Agent å®ç°
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ base.py              # åŸºç¡€ promptï¼ˆæŒ‡ä»¤ã€åŠ¨ä½œåˆ—è¡¨ï¼‰
â”‚   â””â”€â”€ react.py             # ReAct ç‰¹å®šçš„ promptï¼ˆæŒ‡ä»¤ + 2-shot examplesï¼‰
â”œâ”€â”€ environment/
â”‚   â””â”€â”€ alfworld_env.py      # ALFWorld ç¯å¢ƒåŒ…è£…
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ llm.py               # LLM API è°ƒç”¨å’Œå“åº”è§£æ
â”‚   â””â”€â”€ logger.py            # æ—¥å¿—å’Œç»“æœä¿å­˜
â””â”€â”€ run_experiment.py        # ä¸»å®éªŒè„šæœ¬
```

## ğŸ”‘ æ ¸å¿ƒå®ç°ç»„ä»¶

### 1. ReAct Agent ç±» (`agents/react_agent.py`)

**å…³é”®ç‰¹ç‚¹**ï¼š
- ç»§æ‰¿è‡ª `BaseAgent`
- å®ç° `get_system_prompt()` å’Œ `get_instruction()` æ–¹æ³•
- ä½¿ç”¨ 2-shot examplesï¼ˆæ¥è‡ª ReflAct è®ºæ–‡ Appendix K.1.2ï¼‰

**æ ¸å¿ƒä»£ç **ï¼š

```python
class ReActAgent(BaseAgent):
    agent_type = "react"
    
    def get_system_prompt(self) -> str:
        """è¿”å›åŒ…å«æŒ‡ä»¤å’Œ2-shotç¤ºä¾‹çš„å®Œæ•´system prompt"""
        return f"""{SYSTEM_INSTRUCTION}

{REACT_INSTRUCTION}

{AVAILABLE_ACTIONS}

{REMINDER}

Here is an example:
{REACT_EXAMPLE}"""
    
    def get_instruction(self, step: int) -> str:
        """æ¯ä¸ªæ­¥éª¤ä½¿ç”¨ç›¸åŒçš„æŒ‡ä»¤"""
        return REACT_INSTRUCTION
```

### 2. Few-shot Examples (`prompts/react.py`)

**æ¥æº**ï¼šReflAct è®ºæ–‡ Appendix K.1.2ï¼ˆFigure 15 + Figure 17ï¼‰

**ç»“æ„**ï¼š
- **Example 1**: `pick_and_place` ä»»åŠ¡ï¼ˆFigure 15ï¼‰
- **Example 2**: `pick_clean_then_place` ä»»åŠ¡ï¼ˆFigure 17ï¼‰
- **å›ºå®šä½¿ç”¨ 2 ä¸ªç¤ºä¾‹**ï¼ˆä¸åƒ MPO é‚£æ ·æŒ‰ä»»åŠ¡ç±»å‹é€‰æ‹©ï¼‰

**Example 1 ç¤ºä¾‹**ï¼š
```
You are in the middle of a room. Looking quickly around you, you see...
Your task is to: put some spraybottle on toilet.
Thought: To solve the task, I need to find and take a sparybottle...
Action: go to cabinet 1
Observation: On the cabinet 1, you see...
Action: go to cabinet 2
...
```

**å…³é”®æ ¼å¼**ï¼š
- `Thought: ...` â†’ `Action: ...`
- æœ‰æ—¶ç›´æ¥ `Action: ...`ï¼ˆæ²¡æœ‰ Thoughtï¼‰

### 3. System Prompt æ„å»º (`agents/base_agent.py`)

**æ¶ˆæ¯ç»“æ„**ï¼š

```python
messages = [
    {"role": "system", "content": self.get_system_prompt()}  # â­ ä½¿ç”¨ system role
]
```

**System Prompt å†…å®¹**ï¼ˆæ¥è‡ª `react_agent.py`ï¼‰ï¼š
```
[SYSTEM_INSTRUCTION]          # åŸºç¡€æŒ‡ä»¤
[REACT_INSTRUCTION]           # ReAct ç‰¹å®šæŒ‡ä»¤
[AVAILABLE_ACTIONS]           # å¯ç”¨åŠ¨ä½œåˆ—è¡¨
[REMINDER]                    # æé†’äº‹é¡¹
Here is an example:
[REACT_EXAMPLE]               # 2-shot examples
```

**ä¸ MPO çš„åŒºåˆ«**ï¼š
- âœ… **ä½¿ç”¨ `role: "system"`**ï¼ˆMPO å°†æ‰€æœ‰å†…å®¹æ”¾åœ¨ user message ä¸­ï¼‰
- âœ… **å›ºå®šä½¿ç”¨ 2 ä¸ªç¤ºä¾‹**ï¼ˆMPO æŒ‰ä»»åŠ¡ç±»å‹é€‰æ‹©ï¼Œé»˜è®¤ 1 ä¸ªï¼‰

### 4. å¯¹è¯å†å²æ„å»º (`agents/base_agent.py:38-69`)

**User Message æ„å»ºé€»è¾‘**ï¼š

```python
user_content = f"Your task is: {self.task_description}\n"

# æ·»åŠ å†å²è½¨è¿¹ï¼ˆæ¯æ¬¡äº¤äº’éƒ½ä¼šç´¯ç§¯ï¼‰
for step_data in self.trajectory:
    user_content += f"Observation: {step_data['observation']}\n"
    if step_data.get('reasoning'):
        reasoning_type = step_data.get('reasoning_type', 'Thought')
        user_content += f"{reasoning_type.capitalize()}: {step_data['reasoning']}\n"
    user_content += f"Action: {step_data['action']}\n"

# æ·»åŠ å½“å‰è§‚å¯Ÿå’ŒæŒ‡ä»¤
user_content += f"Observation: {observation}\n"
user_content += self.get_instruction(self.step_count + 1)

messages.append({"role": "user", "content": user_content})
```

**ç‰¹ç‚¹**ï¼š
- æ¯æ¬¡è°ƒç”¨éƒ½ä¼šåŒ…å«å®Œæ•´çš„å¯¹è¯å†å²
- æ ¼å¼ï¼š`Observation` â†’ `Thought` â†’ `Action` â†’ `Observation` â†’ ...
- è‡ªåŠ¨ç´¯ç§¯ï¼Œæ— éœ€æ‰‹åŠ¨ç®¡ç†å†å²

### 5. LLM è°ƒç”¨ (`utils/llm.py`)

**API è°ƒç”¨**ï¼š

```python
def call_llm(messages: list, temperature: float = None) -> str:
    client = OpenAI(
        api_key=config.OPENAI_API_KEY,
        base_url=config.OPENAI_BASE_URL  # æ”¯æŒä»£ç†/ä¸­è½¬
    )
    
    response = client.chat.completions.create(
        model=config.OPENAI_MODEL,
        messages=messages,
        temperature=temperature or 0,  # é»˜è®¤è´ªå©ªè§£ç 
        max_tokens=512,
    )
    
    return response.choices[0].message.content.strip()
```

**å“åº”è§£æ**ï¼š

```python
def parse_thought_or_reflection(response: str) -> tuple:
    """è§£æ Thought/Reflection å’Œ Action"""
    # å°è¯•åŒ¹é… Thought:
    thought_match = re.search(r'Thought:\s*(.+?)(?=Action:|$)', response, re.IGNORECASE | re.DOTALL)
    
    # å°è¯•åŒ¹é… Action:
    action = parse_action(response)
    
    return reasoning_type, reasoning_content, action
```

### 6. ç¯å¢ƒäº¤äº’ (`environment/alfworld_env.py`)

**åˆå§‹åŒ–**ï¼š

```python
class ALFWorldEnv:
    def __init__(self, split: str = "eval_out_of_distribution"):
        # ä½¿ç”¨å®˜æ–¹ ALFWorld API
        from alfworld.agents.environment import get_environment
        
        config = generic.load_config()  # ä» configs/base_config.yaml åŠ è½½
        env_type = config['env']['type']
        
        self.env = get_environment(env_type)(config, train_eval=split)
        self.env = self.env.init_env(batch_size=1)
```

**Reset æ–¹æ³•**ï¼š

```python
def reset(self, task_idx: Optional[int] = None) -> Tuple[str, str]:
    obs, info = self.env.reset()
    
    # è§£æè§‚å¯Ÿå’Œä»»åŠ¡æè¿°
    observation = obs[0] if isinstance(obs, list) else obs
    task_description = info.get('extra.goal', [""])[0]
    
    return observation, task_description
```

**Step æ–¹æ³•**ï¼š

```python
def step(self, action: str) -> Tuple[str, float, bool, Dict]:
    obs, scores, dones, infos = self.env.step([action])
    
    observation = obs[0] if isinstance(obs, list) else obs
    reward = scores[0] if isinstance(scores, list) else scores
    done = dones[0] if isinstance(dones, list) else dones
    
    return observation, reward, done, infos
```

### 7. ä¸»æ‰§è¡Œå¾ªç¯ (`agents/base_agent.py:132-171`)

```python
def run_task(self, env, task_id: int = 0) -> Tuple[bool, List[Dict]]:
    # 1. é‡ç½®ç¯å¢ƒå’Œ Agent
    obs, task_desc = env.reset()
    self.reset(task_desc)
    
    # 2. ReAct å¾ªç¯ï¼ˆæœ€å¤š MAX_STEPS æ­¥ï¼‰
    while not done and self.step_count < config.MAX_STEPS:
        action = self.step(obs)           # Agent ç”ŸæˆåŠ¨ä½œ
        obs, reward, done, info = env.step(action)  # ç¯å¢ƒæ‰§è¡Œ
        
        if done:
            success = reward > 0
    
    return success, self.trajectory
```

**Step æ–¹æ³•**ï¼ˆå•æ­¥æ‰§è¡Œï¼‰ï¼š

```python
def step(self, observation: str) -> str:
    self.step_count += 1
    
    # 1. ç”ŸæˆåŠ¨ä½œï¼ˆåŒ…å« Thoughtï¼‰
    action, reasoning_type, reasoning = self.act(observation)
    
    # 2. è®°å½•è½¨è¿¹
    step_data = {
        "step": self.step_count,
        "observation": observation,
        "action": action,
        "reasoning_type": reasoning_type,
        "reasoning": reasoning,
    }
    self.trajectory.append(step_data)
    
    return action
```

## ğŸ“Š ä¸ MPO å®ç°çš„å¯¹æ¯”

| ç‰¹æ€§ | ReflAct (reflectact) | MPO |
|------|---------------------|-----|
| **System Prompt** | âœ… ä½¿ç”¨ `role: "system"` | âŒ æ‰€æœ‰å†…å®¹åœ¨ user message ä¸­ |
| **Few-shot ç¤ºä¾‹** | å›ºå®š 2 ä¸ªï¼ˆæ¥è‡ªè®ºæ–‡ï¼‰ | æŒ‰ä»»åŠ¡ç±»å‹é€‰æ‹©ï¼Œé»˜è®¤ 1 ä¸ª |
| **ç¤ºä¾‹æ¥æº** | è®ºæ–‡ Figure 15 + 17 | `alfworld_icl.json`ï¼ˆæŒ‰ä»»åŠ¡ç±»å‹ç»„ç»‡ï¼‰ |
| **å¯¹è¯å†å²** | è‡ªåŠ¨ç´¯ç§¯åœ¨ user message | è‡ªåŠ¨ç´¯ç§¯åœ¨ `state.history` |
| **Action è§£æ** | æ­£åˆ™è¡¨è¾¾å¼æå– | æ­£åˆ™è¡¨è¾¾å¼æå– |
| **ç¯å¢ƒåŒ…è£…** | å®˜æ–¹ ALFWorld API | å®˜æ–¹ ALFWorld API |
| **é…ç½®æ–¹å¼** | `.env` æ–‡ä»¶ + `config.py` | JSON é…ç½®æ–‡ä»¶ |

## ğŸ”„ å®Œæ•´æ‰§è¡Œæµç¨‹

```
1. åˆå§‹åŒ–
   â”œâ”€ åˆ›å»º ALFWorldEnv
   â”œâ”€ åˆ›å»º ReActAgent
   â””â”€ Agent æ„å»º system promptï¼ˆåŒ…å« 2-shot examplesï¼‰

2. ä»»åŠ¡å¾ªç¯ï¼ˆrun_taskï¼‰
   â”œâ”€ env.reset()
   â”‚  â””â”€ è¿”å› (observation, task_description)
   â”‚
   â”œâ”€ agent.reset(task_description)
   â”‚  â””â”€ æ¸…ç©º trajectory
   â”‚
   â””â”€ ReAct å¾ªç¯ï¼ˆæœ€å¤š 50 æ­¥ï¼‰
      â”œâ”€ agent.step(obs)
      â”‚  â”œâ”€ build_messages(obs)
      â”‚  â”‚  â”œâ”€ system: [instruction + 2-shot examples]
      â”‚  â”‚  â””â”€ user: [task + history + current_obs + instruction]
      â”‚  â”‚
      â”‚  â”œâ”€ call_llm(messages)
      â”‚  â”‚  â””â”€ è¿”å› "Thought: ...\n Action: ..."
      â”‚  â”‚
      â”‚  â”œâ”€ parse_thought_or_reflection(response)
      â”‚  â”‚  â””â”€ æå– thought å’Œ action
      â”‚  â”‚
      â”‚  â””â”€ æ›´æ–° trajectory
      â”‚
      â”œâ”€ env.step(action)
      â”‚  â””â”€ è¿”å› (observation, reward, done, info)
      â”‚
      â””â”€ æ£€æŸ¥æ˜¯å¦å®Œæˆæˆ–è¶…æ—¶
```

## ğŸ“ Prompt ç»“æ„ç¤ºä¾‹

### System Messageï¼ˆç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶è®¾ç½®ï¼‰

```
Interact with a household to solve a task...

For each of your turn, you will be given the observation...
Your output must strictly follow this format: "Thought: your thoughts.\n Action: your next action"

The available actions are:
1. go to recep
2. take obj from recep
...

Here is an example:
Example 1:
[å®Œæ•´çš„ 2-shot ç¤ºä¾‹ 1]
Example 2:
[å®Œæ•´çš„ 2-shot ç¤ºä¾‹ 2]
```

### User Messageï¼ˆæ¯æ¬¡è°ƒç”¨æ—¶æ„å»ºï¼‰

```
Your task is: put some spraybottle on toilet

Observation: You are in the middle of a room...
Thought: To solve the task, I need to...
Action: go to cabinet 1

Observation: On the cabinet 1, you see...
Action: go to cabinet 2

...

Observation: [å½“å‰è§‚å¯Ÿ]
For each of your turn, you will be given the observation...
```

## ğŸ¯ å…³é”®è®¾è®¡ç‰¹ç‚¹

### 1. **æ¨¡å—åŒ–è®¾è®¡**
- Agentã€Promptã€Environment å®Œå…¨åˆ†ç¦»
- æ˜“äºæ‰©å±•æ–°çš„ Agent ç±»å‹

### 2. **æ ‡å‡† OpenAI API æ ¼å¼**
- ä½¿ç”¨æ ‡å‡†çš„ `messages` æ ¼å¼
- System/User è§’è‰²æ¸…æ™°åˆ†ç¦»

### 3. **è‡ªåŠ¨å†å²ç®¡ç†**
- `trajectory` è‡ªåŠ¨ç´¯ç§¯æ‰€æœ‰æ­¥éª¤
- æ¯æ¬¡è°ƒç”¨ LLM æ—¶è‡ªåŠ¨åŒ…å«å®Œæ•´å†å²

### 4. **çµæ´»çš„é…ç½®**
- æ”¯æŒç¯å¢ƒå˜é‡å’Œ `.env` æ–‡ä»¶
- æ”¯æŒä»£ç† APIï¼ˆå›½å†…å¯ç”¨ï¼‰

### 5. **Mock æ¨¡å¼æ”¯æŒ**
- å¯ä»¥åœ¨æ²¡æœ‰ ALFWorld çš„æƒ…å†µä¸‹æµ‹è¯•
- ä¾¿äºå¼€å‘å’Œè°ƒè¯•

## ğŸ” å…³é”®ä»£ç ä½ç½®

| åŠŸèƒ½ | æ–‡ä»¶ | å…³é”®æ–¹æ³•/å˜é‡ |
|------|------|--------------|
| **ReAct Agent** | `agents/react_agent.py` | `get_system_prompt()`, `get_instruction()` |
| **Few-shot ç¤ºä¾‹** | `prompts/react.py` | `REACT_EXAMPLE_1`, `REACT_EXAMPLE_2` |
| **åŸºç¡€æŒ‡ä»¤** | `prompts/base.py` | `SYSTEM_INSTRUCTION`, `AVAILABLE_ACTIONS` |
| **æ¶ˆæ¯æ„å»º** | `agents/base_agent.py` | `build_messages()` |
| **æ‰§è¡Œå¾ªç¯** | `agents/base_agent.py` | `run_task()`, `step()` |
| **LLM è°ƒç”¨** | `utils/llm.py` | `call_llm()`, `parse_thought_or_reflection()` |
| **ç¯å¢ƒåŒ…è£…** | `environment/alfworld_env.py` | `reset()`, `step()` |
| **ä¸»å®éªŒ** | `run_experiment.py` | `run_agent_experiments()` |

## ğŸš€ è¿è¡Œæ–¹å¼

```bash
# è¿è¡Œ ReAct Agent
python run_experiment.py --agent react --num_tasks 10

# è¿è¡Œæ‰€æœ‰æ–¹æ³•å¯¹æ¯”
python run_experiment.py --agent all --num_tasks 134
```

## ğŸ“ˆ é¢„æœŸç»“æœ

æ ¹æ® ReflAct è®ºæ–‡ï¼ˆGPT-4o-miniï¼‰ï¼š
- **ReAct**: 53.0% æˆåŠŸç‡
- **ReflAct**: 66.4% æˆåŠŸç‡

## ğŸ”‘ å…³é”®æ´å¯Ÿ

1. **å›ºå®šçš„ 2-shot ç¤ºä¾‹**ï¼šä¸ MPO ä¸åŒï¼Œä¸æŒ‰ä»»åŠ¡ç±»å‹é€‰æ‹©ï¼Œè€Œæ˜¯ä½¿ç”¨è®ºæ–‡ä¸­çš„ä¸¤ä¸ªå›ºå®šç¤ºä¾‹
2. **System Prompt ä½¿ç”¨**ï¼šæ˜ç¡®ä½¿ç”¨ system roleï¼Œä½¿æŒ‡ä»¤å’Œç¤ºä¾‹ä¸å¯¹è¯å†å²åˆ†ç¦»
3. **è‡ªåŠ¨å†å²ç´¯ç§¯**ï¼šæ¯æ¬¡è°ƒç”¨éƒ½ä¼šåŒ…å«å®Œæ•´å†å²ï¼Œæ— éœ€æ‰‹åŠ¨ç®¡ç†
4. **æ¸…æ™°çš„æ¨¡å—åŒ–**ï¼šä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œæ‰©å±•

